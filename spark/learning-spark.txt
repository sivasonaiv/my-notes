Learning Spark - notes

Spark
General purpose framework for cluster computing

Components

- Spark Core
	API that defines RDD
    RDD- represents collection that distributes across many compute nodes that can be manipulated in parallel

- Spark SQL
	SQL interface to Spark

- Spark Streaming
- Spark MLlib
- Spark GraphX
	library to manipulate graphs and performing graph-parallel computations

- Cluster managers

Installation
     Spark 2.1.0
     Scala 2.11.8
     Python 2.7.12

RDD

- An RDD in Spark is simply an immutable distributed collection of objects.
- Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster.
- RDDs can contain any type of Python, Java, or Scala objects, including user- defined classes.
- Users create RDDs in two ways: by loading an external dataset, or by distributing a collection of objects (e.g., a list or set) in their driver program (sc.parallelize())
- Once created, RDDs offer two types of operations: transformations and actions. Transformations construct a new RDD from a previous one. [Transformations return RDDs, whereas actions return some other data type.]
- loads data only when its used in actions
- Sparkâ€™s RDDs are by default recomputed each time you run an action on them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using RDD.persist()

To summarize, every Spark program and shell session will work as follows:

- Create some input RDDs from external data.
- Transform them to define new RDDs using transformations like filter().
- Ask Spark to persist() any intermediate RDDs that will need to be reused.
- Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and executed by Spark.

As RDDs are immutable, transformation every time creates new RDD and keep track of the dependencies between RDDs
Spark keeps track of the set of dependencies between different RDDs, called the lineage graph.
Transformation - Lazy Evaluation - added to the DAG structure without actually executing
Action - Early Evaluation - executed immediately

sc.parllalize() -> method used to convert given collection into RDD

filter() - its transformation method that returns new RDD

eg: val inputRDD = sc.textFile("log.txt")
	val errorsRDD = inputRDD.filter(line => line.contains("error"))
	val warningRDD = inputRDD.filter(line => line.contains("warning"))
	val badlinesRDD = errorsRDD.union(warningsRDD)
	
union() -> takes two rdds and returns new RDD with contents of both (will contain duplicates)

Spark keeps track of the set of dependencies between different RDDs - called the lineage graph

count() -> it returns total no of elements in an RDD
take() -> it returns given no of elements from an RDD

collect() -> to get the entire RDD into Memory. shouldnt use for large dataset
---
passing functions to spark
	target function that has to be operated on each element in the RDD can be passed as lambda argument
	
python example
	
def containsError(s):
	return "error" in s
word = rdd.filter(containsError)

One issue to watch out for when passing functions is inadvertently serializing the object containing the function.
---
Common Transformations and actions used
 
 map() and filter() are two transformation commonly used
 
 map() - takes in a function and applies it to each element in RDD with the result of the function being
 		 the new value of each element in the resulting RDD.
 		 
 filter() - takes in a function and returns RDD that only has elements that pass the filter() function
 flatMap() - called individually for each element in RDD and returns an iterator with our return values
 
 RDD.distinct() - to produce a new RDD with only distinct items
 
 intersection() - returns only elements in both RDDs - also removes all duplicates
 
 example:
 
 RDD 1 - {coffee, coffee, panda, monkey, tea}
 RDD 2 - {coffee, monkey, kitty}
 
 RDD1.distinct() - {coffee, panda, monkey, tea }
 RDD1.union(RDD2) - {coffee,coffee,coffee,panda,monkey,monkey,tea,kitty}
 RDD1.intersection(RDD2) - {coffee,monkey}
 RDD1.subtract(RDD2) - {panda,tea}
 
 
 cartesian(other) -  transformation returns all possible pairs of (a, b) 
 					where a is in the source RDD and b is in the other RDD
 
 
 
 
 
 
 
 
 
 
 
 
 
	




	
